{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "figured-forum",
   "metadata": {},
   "source": [
    "### Linear regression model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "random-liabilities",
   "metadata": {},
   "source": [
    "A linear model makes a prediction by computing a **weighted sum of the input features** plus a constant called the **bias term** (or intercept term): \n",
    "\n",
    "$\\hat{y}$ = $\\theta_{0}$ + $\\theta_{1}$$x_{1}$ + $\\theta_{2}$$x_{2}$ + ... + $\\theta_{n}$$x_{n}$\n",
    "\n",
    " * $\\hat{y}$ = predicted value \n",
    " * n = number of features \n",
    " * $x_{i}$ = i-th feature value \n",
    " * $\\theta_{j}$ = j-th model parameter, including the bias term $\\theta_{0}$ and the feature weights $\\theta_{1}$ ... $\\theta_{n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boolean-ordering",
   "metadata": {},
   "source": [
    "In **vectorized form**, the linear regression model prediction is: \n",
    "\n",
    "$\\hat{y}$ = $h_{\\theta}$(x) = $\\theta$x\n",
    "\n",
    " * $\\theta$ = model's parameter vector, containing the bias term $\\theta_{0}$ and the feature weights $\\theta_{1}$ ... $\\theta_{n}$\n",
    " * x = instances' feature vector, containing $x_{0}$ to $x_{n}$, with $x_{0}$ always equal to 1 \n",
    " * $\\theta$x = dot product of the vectors $\\theta$ and x \n",
    " * $h_{\\theta}$ = hypothesis function using the model parameters $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nonprofit-burning",
   "metadata": {},
   "source": [
    "### Training a linear regression model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrapped-david",
   "metadata": {},
   "source": [
    "#### 1. Cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "horizontal-roots",
   "metadata": {},
   "source": [
    "Cost function: Mean Squared Error (MSE)\n",
    "\n",
    "MSE(X, $h_{\\theta}$) = $\\frac{1}{2}$ $\\sum_{i = 1}^{m}$ ($\\theta$$x^{(i)}$ - $y^{(i)}$)$^{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "primary-migration",
   "metadata": {},
   "source": [
    "To minimize the MSE, we can use: \n",
    "\n",
    " * **Closed-form equation (Normal equation)** = directly computes the model parameters that best fit the model to the training set (model parameters that minimize the MSE over the training set) \n",
    " * **Iterative approach** called **Gradient Descent** = gradually tweaks the model parameters to minimize the cost function over the training set, eventually converging to the same set of parameters as the Normal equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pregnant-shanghai",
   "metadata": {},
   "source": [
    "#### 2. Normal equation (closed-form solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooked-density",
   "metadata": {},
   "source": [
    "$\\hat{\\theta}$ = ($X^{T}$ $X$)$^{-1}$ $X^{T}$ $y$\n",
    "\n",
    " * $\\hat{\\theta}$ = value of $\\theta$ that minimizes the cost function \n",
    " * y = vector of target values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dress-friendship",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate linear-looking data \n",
    "import numpy as np \n",
    "\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1) # Gaussian noise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "chicken-services",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_b = np.c_[np.ones((100, 1)), X] # add x0 = 1 to each instance \n",
    "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "unknown-landscape",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.13585256],\n",
       "       [2.84301161]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_best # not too far from the real intercept and slope of the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "soviet-syria",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.13585256],\n",
       "       [9.82187578]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions using the parameter vector computed with the closed form solution \n",
    "X_new = np.array([[0], [2]])\n",
    "X_new_b = np.c_[np.ones((2, 1)), X_new] # add x0 = 1 to each instance \n",
    "y_predict = X_new_b.dot(theta_best)\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "floral-cheat",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.plot(X_new, y_predict, \"r-\")\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.axis([0, 2, 0, 15]) # x and y min and max coordinates \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "current-arrival",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.13585256]), array([[2.84301161]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in sklearn \n",
    "from sklearn.linear_model import LinearRegression \n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "controlled-rainbow",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.13585256],\n",
       "       [9.82187578]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facial-sequence",
   "metadata": {},
   "source": [
    "LinearRegression class in sklearn is based on the scipy.linalg.lstsq() function, which computes \n",
    "\n",
    "$\\hat{\\theta}$ = ($X^{+}$ $y$)\n",
    "\n",
    " * $X^{+}$ = pseudoinverse of X, which is computed using Singular Value Decomposition. SVD decomposes the training set $X$ into the matrix multiplication of $U$ $\\sum_{}^{+}$ $V^{T}$ --> this is useful because the Normal equation may not work of $X^{T}$ $X$ is not invertible, such as when m < n or if some features are redundant. On the other hand, the pseudoinverse of a matrix is always defined. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "listed-pharmacology",
   "metadata": {},
   "source": [
    "#### 3. Gradient descent "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excited-actress",
   "metadata": {},
   "source": [
    "Iterative procedure to tweak parameters in order to minimize a cost function. It measures the local gradient of the cost function with respect to the parameter vector $\\theta$ and it goes in the direction of the descending gradient. Once the gradient is 0, we have reached a minimum. \n",
    "\n",
    " 1. Start by filling $\\theta$ with random values (random initialization) \n",
    " 2. Improve $\\theta$ gradually at each step attempting to decrease the cost function \n",
    " 3. Algorithm converges to a minimum \n",
    " \n",
    "Important hyperparameter in Gradient Descent is the size of the steps: **learning rate**\n",
    "\n",
    "MSE cost function for a Linear Regression model is a convex function, meaning that if we pick any two points on the curve, the line segment joining them never crosses the curve. --> there are no local minima, just one global minimum. It is also a continuous function with a slope that never changes abruptly. Because of these two features, Gradient Descent is guaranteed to approach arbitrarily close the global minimum. \n",
    "\n",
    "**!!** When using Gradient Descent, we should ensure that all features have a similar scale or it will take much longer to converge. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silent-sheep",
   "metadata": {},
   "source": [
    "#### 3a. Batch Gradient Descent \n",
    "\n",
    "To implement Gradient Descent, we need to compute the gradient of the cost function with respect to each model parameter $\\theta_{j}$ (partial derivatives). For MSE, the partial derivatives are: \n",
    "\n",
    "$\\displaystyle \\frac{\\partial MSE(\\theta)}{\\partial \\theta_{j}}$ =  $\\frac{2}{m}$ $\\sum_{i = 1}^{m}$ ($\\theta$$x^{(i)}$ - $y^{(i)}$)$x_{j}^{(i)}$\n",
    "\n",
    "Instead of computing these partial derivatives individually, we can instead compute them in one go using a gradient vector which contains all the partial derivatives of the cost function (one for each model parameter): \n",
    "\n",
    "$\\nabla_{\\theta}$ MSE($\\theta$) = $\\frac{2}{m}$ $X^{T}$ ($X$ $\\theta$ - $y$)\n",
    "\n",
    "This notation involves calculations over the full training set $X$ at every step. As a result, Batch Gradient Descent is very slow on large trianing sets but scales well with the number of features. \n",
    "\n",
    "Once we have the gradient vector, which by definition points uphill, we go in the opposite direction to go downhill. This means subtracting $\\nabla_{\\theta}$ MSE($\\theta$) from $\\theta$. We also multiply the gradient vector by the learning rate $\\eta$ to determine the size of the downhill step. \n",
    "\n",
    "$\\theta^{(next step)}$ = $\\theta$ - $\\eta$ $\\nabla_{\\theta}$ MSE($\\theta$) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "third-queensland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.13585256],\n",
       "       [2.84301161]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta = 0.1 # Specify learning rate \n",
    "n_iterations = 1000\n",
    "m = 100\n",
    "\n",
    "theta = np.random.randn(2, 1) # random initialization of the parameters \n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta = theta - eta * gradients\n",
    "\n",
    "theta # same as what the normal equation found!! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunrise-blast",
   "metadata": {},
   "source": [
    "#### 3b. Stochastic Gradient Descent \n",
    "\n",
    "The main problem with Batch Gradient Descent is that it uses the whole training set to compute gradients at every step, which makes it slow as the dataset gets large. Stochastic Gradient Descent, instead, picks a random instance in the training set at every step and computes the gradients based only on that single instance. \n",
    "\n",
    " * faster implementation \n",
    " * stochasticy makes it less regular than Batch Gradient Descent \n",
    " * finds good but not optimal parameter values\n",
    " * when the cost function is very irregular, it can help the algorithm jump out of local minima \n",
    " * to settle at the minimum, we can gradually reduce the learning rate by setting a **learning schedule**, which determines the learning rate at each iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "based-memphis",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50 \n",
    "t0, t1 = 5, 50 # learning schedule hyperparameters \n",
    "\n",
    "def learning_schedule(t):\n",
    "    return t0 / (t + t1)\n",
    "\n",
    "theta = np.random.randn(2, 1) # random initialization of model parameters \n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        random_index = np.random.randint(m)\n",
    "        xi = X_b[random_index: random_index+1]\n",
    "        yi = y[random_index: random_index+1]\n",
    "        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
    "        eta = learning_schedule(epoch * m + i)\n",
    "        theta = theta - eta * gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "frank-target",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.08471239],\n",
       "       [2.78775735]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fewer-revolution",
   "metadata": {},
   "source": [
    "**!!** Since the instances are picked randomly, some instances may be picked several times per epoch while others may not be picked at all. If we want to be sure that the algorithm goes through every instance at each epoch, we can **shuffle the training set**, then go through it instance by instance, then shuffle it again... Shuffling is important because it makes sure that instances are **independent and identically distributed (IID)** --> otherwise, if the instances are for example sorted by label (hence not IID), SGD will start by optimizing for one label, then the next and so on without ever settling close to the global minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "broad-queens",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDRegressor(eta0=0.1, penalty=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor \n",
    "sgd_reg = SGDRegressor(max_iter = 1000, tol = 1e-3, penalty=None, eta0=0.1)\n",
    "sgd_reg.fit(X, y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "violent-delivery",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.1860683]), array([2.99340302]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_reg.intercept_, sgd_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "objective-external",
   "metadata": {},
   "source": [
    "#### 3c. Mini-batch Gradient Descent\n",
    "\n",
    "At each step, instead of computing the gradients based on the full training set or based on just one instance, Mini-batch Gradient Descent computes the gradients on a small random set of instances called a mini-batch. \n",
    "\n",
    " * progress in the parameter space is less erratic that with SGD "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fuzzy-collaboration",
   "metadata": {},
   "source": [
    "#### End of notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fallen-isaac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scanpy_env",
   "language": "python",
   "name": "scanpy_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
