{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "corrected-basis",
   "metadata": {},
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silent-chaos",
   "metadata": {},
   "source": [
    "#### 1. Linear SVM Classification\n",
    "\n",
    "Two classes are **linearly separable** if they can be clearly separated with a straight line. SVM classifiers fit the widest possible \"street\" between the classes (**large margin** classification). \n",
    "\n",
    " * Adding more training instances \"off the street\" does not affect the decision boundary at all --> it is fully determined (or \"supported\") by the instances located on the edge of the street, which are called **support vectors** \n",
    " \n",
    " * SVMs are **sensitive to feature scales**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interesting-apple",
   "metadata": {},
   "source": [
    "#### 1a. Hard Margin Classification\n",
    "\n",
    "If we strictly impose that all instances must be off the street, this is called hard margin classification, which has two issues: \n",
    "\n",
    " * Only works if the data is linearly separable \n",
    " * Sensitive to outliers\n",
    " \n",
    "\n",
    "#### 1b. Soft Margin Classification \n",
    "\n",
    "To avoid these issues, we can modify the goal to find a **good balance between keeping the street as large as possible and limiting the margin violations**. \n",
    "\n",
    " * Overfitting can be reduced by regularizing the model by reducing C "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "precise-equity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "from sklearn import datasets \n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.svm import LinearSVC \n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris['data'][:, (2,3)] # petal length and petal width\n",
    "y = (iris['target'] == 2).astype(np.float64) # Iris virginica \n",
    "\n",
    "svm_clf = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    (\"linear_svc\", LinearSVC(C = 1, loss = 'hinge'))\n",
    "])\n",
    "svm_clf.fit(X, y)\n",
    "svm_clf.predict([[5.5, 1.7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verified-plane",
   "metadata": {},
   "source": [
    " * Unlike Logistic Regression classifiers, SVM classifiers **do not output probabilities** for each class unless specified by a parameter. \n",
    " * LinearSVC class regularizes the bias term, so we should **center the training set** by subtracting its mean --> this is automatic if we scale the data using the StandardScaler. \n",
    " * Specify the **loss = \"hinge\"**\n",
    " * Set **dual = False** unless there are more features than training instances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "super-owner",
   "metadata": {},
   "source": [
    "#### 2. Nonlinear SVM Classification \n",
    "\n",
    "Adding features (eg. polynomial features) can make nonlinear datasets linearly separable. To implement this idea, we can create a Pipeline containing a PolynomialFeatures transformer, followed by a StandardScaler and LinearSVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "formal-jumping",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/my-conda-envs/scanpy_env/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('poly_features', PolynomialFeatures(degree=3)),\n",
       "                ('scaler', StandardScaler()),\n",
       "                ('svm_clf', LinearSVC(C=10, loss='hinge'))])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_moons \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures \n",
    "\n",
    "X, y = make_moons(n_samples = 100, noise = 0.15)\n",
    "polynomial_svm_clf = Pipeline([\n",
    "    ('poly_features', PolynomialFeatures(degree = 3)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm_clf', LinearSVC(C = 10, loss = 'hinge'))\n",
    "])\n",
    "\n",
    "polynomial_svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concrete-mainstream",
   "metadata": {},
   "source": [
    "#### 2a. Polynomial kernel \n",
    "\n",
    "Adding polynomial features is not necessarily always possible because at low polynomial degree the model cannot deal with complex datasets, and at high polynomial degree the model becomes very slow. \n",
    "\n",
    "--> When using SVMs we can apply the **kernel trick**, which allows to get the same results as if we had added many polynomial features without actually having to add them. This means that there is no combinatorial explosion of the number of features because we don't actually add any features. \n",
    "\n",
    "In other words, a **kernel** is a function capable of computing the dot product $\\phi(a)^{T}$ $\\phi(b)$ based only on the original vectors $a$ and $b$, without having to compute (or even know) the transformation $\\phi$.\n",
    "\n",
    "Most commonly used kernels are: \n",
    "\n",
    " * Linear: $K(a, b)$ = $a^{T}b$\n",
    " * Polynomial: $K(a, b)$ = $(\\gamma a^{T}b + r)^{d}$\n",
    " * Gaussian RBF: $K(a, b)$ = exp(-$\\gamma||a - b||^{2}$\n",
    " * Sigmoid: $K(a, b)$ = tanh $(\\gamma a^{T}b + r)$\n",
    " \n",
    " \n",
    "\n",
    "According to **Mercer's theorem**, if a function $K(a, b)$ respects conditions of continuity and symmetry in its arguments, then there exists a function $\\phi$ that maps $a$ and $b$ into another space (possibly which much higher dimensions) such that $K(a, b)$ = $\\phi(a)^{T}$ $\\phi(b)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "czech-girlfriend",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('svm_clf', SVC(C=5, coef0=1, kernel='poly'))])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC \n",
    "\n",
    "poly_kernel_svm_clf = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm_clf', SVC(kernel = 'poly', degree = 3, coef0 = 1, C = 5))\n",
    "])\n",
    "\n",
    "poly_kernel_svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "through-exposure",
   "metadata": {},
   "source": [
    "#### 2b. Similarity features \n",
    "\n",
    "Another technique to tackle nonlinear problems is to add features computed using a **similarity function**, which measures how much each instance resembles a particular **landmark**.\n",
    "Ex. **Gaussian Radial Basis Function (RBF)** \n",
    "\n",
    "$\\phi_{\\gamma}(x, l)$ = exp(-$\\gamma||x - l||^{2}$)\n",
    "\n",
    " * $\\gamma$ is a hyperparameter \n",
    " * $l$ is the landmark, which we can choose to be the location of each and every instance in the dataset. This creates many dimensions and thus increases the chances that the transformed training set will be linearly separable. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocational-hungary",
   "metadata": {},
   "source": [
    "#### 2c. Gaussian RBF kernel \n",
    "\n",
    "The **kernel trick** can also be applied to similarity features methods! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "hollow-marine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('svm_clf', SVC(C=0.001, gamma=5))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbf_kernel_svm_clf = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm_clf', SVC(kernel = 'rbf', gamma = 5, C = 0.001))\n",
    "])\n",
    "\n",
    "rbf_kernel_svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-front",
   "metadata": {},
   "source": [
    " * Increasing $\\gamma$ makes the bell-shaped curve narrower --> decision boundary is more irregular, wiggling around individual instances \n",
    " * $\\gamma$ acts as a regularization paramenter: if the model is overfitting, we should reduce it: if it's underfitting, we should increase it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comic-prefix",
   "metadata": {},
   "source": [
    "#### 3. SVM Regression\n",
    "\n",
    "To use SVMs for regression instead of classification, the objective is to fit as many instances as possible **on the street** while limiting margin violations. The width of the stree is controlled by the parameter $\\epsilon$\n",
    "\n",
    " * Adding more training instances within the margin does not affect the model's predictions --> $\\epsilon$ insensitive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "powerful-bermuda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVR(epsilon=1.5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVR \n",
    "\n",
    "svm_reg = LinearSVR(epsilon=1.5)\n",
    "svm_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "antique-syndrome",
   "metadata": {},
   "source": [
    "To tackle nonlinear regression tasks, we can use a kernelized SVM model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "chubby-island",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=100, degree=2, kernel='poly')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVR \n",
    "\n",
    "svm_poly_reg = SVR(kernel = 'poly', degree = 2, C = 100, epsilon = 0.1)\n",
    "svm_poly_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "touched-soviet",
   "metadata": {},
   "source": [
    "#### 4. Hinge loss \n",
    "\n",
    "$max(0, 1-t)$ is called the Hinge loss. \n",
    "\n",
    " * equal to 0 when t >= 1\n",
    " * derivative is eqaul to -1 if t < 1 and 0 if t > 1\n",
    " * not differentiable at t = 1, but like Lasso we can use subgradients when using Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "maritime-showcase",
   "metadata": {},
   "source": [
    "#### End of notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noted-soccer",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scanpy_env",
   "language": "python",
   "name": "scanpy_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
