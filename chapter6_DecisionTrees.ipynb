{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "worth-circumstances",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infinite-highland",
   "metadata": {},
   "source": [
    "Decision Trees are versatile machine learning models that can perform both classification and regression tasks, including multioutput tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "satellite-democrat",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=2)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris \n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, 2:] # petal length and width \n",
    "y = iris.target\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(max_depth = 2)\n",
    "tree_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "covered-humanitarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize trained Decision Tree with graphviz \n",
    "from sklearn.tree import export_graphviz \n",
    "\n",
    "export_graphviz(\n",
    "tree_clf, \n",
    "out_file = \"./iris_tree.dot\",\n",
    "feature_names = iris.feature_names[2:],\n",
    "class_names = iris.target_names,\n",
    "rounded = True, \n",
    "filled = True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "taken-beginning",
   "metadata": {},
   "source": [
    "sudo apt install graphviz\n",
    "dot -Tpdf iris_tree.dot -o iris_tree.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiac-sensitivity",
   "metadata": {},
   "source": [
    "**!** Unlike LogReg and SVMs, Decision Trees don't require feature scaling or centering at all\n",
    "\n",
    "**Node attributes**: \n",
    " * samples = how many training instances it applies to \n",
    " * value = how many training instances of each class in this node applies to \n",
    " * gini = impurity of the node. A node is considered pure (gini = 0) if all training instances it applies to belong to the same class. \n",
    " \n",
    "\n",
    "**Gini impurity**\n",
    "\n",
    "\n",
    "$G_{i}$ = 1 - $\\sum_{k = 1}^{n}$ $p_{i,k}^{2}$\n",
    "\n",
    "$p_{i,k}$ = ratio of class k instances among the training instances in the i-th node.\n",
    "\n",
    "**!** sklearn uses the CART algorithm which produces only binary trees (ie. nonleaf nodes always have two children). Other algorithms such as ID3 can produced Decision Trees with nodes that have more than two children.\n",
    "\n",
    "\n",
    "**Estimating class probabilities** \n",
    "\n",
    "Decision Trees can also estimate the probability that an instance belongs to a particular class k. It first traverses the tree to find the leaf node for this instance, and then it returns the ratio of training instances of class k in this node. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "demonstrated-gauge",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.90740741, 0.09259259]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.predict_proba([[5, 1.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "decimal-explanation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.predict([[5, 1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-department",
   "metadata": {},
   "source": [
    "#### CART Training Algorithm for Classification\n",
    "\n",
    "1. Split the training set into two subsets using a single feature k and a threshold $t{k}$ --> t and k are chosen so as to produce the purest subsets (weighted by their size). \n",
    "\n",
    "**Cost function for classification** \n",
    "\n",
    "$J(k, t_{k})$ = $\\frac{m_{left}}{m}$ $G_{left}$ + $\\frac{m_{right}}{m}$ $G_{right}$\n",
    "\n",
    " * $G_{left/right}$ = measures the impurity of the left/right subset \n",
    " * $m_{left/right}$ = number of instances in the left/right subset\n",
    " \n",
    "2. Split the two subsets using the same logic, recursively.\n",
    "\n",
    "3. Stop recursion when it reaches the maximum depth (defined by the max_depth hyperparameter) or if it cannot find a split that will reduce impurity. \n",
    "\n",
    "CART is a greedy algorithm because it greedily searches for an optimum split at the top level, then repeats the process at each subsequent level. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peaceful-cooper",
   "metadata": {},
   "source": [
    "#### Gini impurity or Entropy\n",
    "\n",
    "Gini impurity is the default, but can be substituted by entropy (criterion = 'entropy'). \n",
    "\n",
    "$H_{i}$ = - $\\sum_{k = 1}^{n}$ $p_{i,k}log_{2}(p_{i,k})$\n",
    "\n",
    "A set's entropy is 0 if it contains instances of only one class. \n",
    "\n",
    " * Gini impurity is faster to compute but tends to isolate the most frequent class in its own branch of the tree. \n",
    " \n",
    " * Entropy tends to produce slightly more balanced trees. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wicked-prisoner",
   "metadata": {},
   "source": [
    "#### Regularization hyperparameters \n",
    "\n",
    "Decision Trees many almost no assumption about the data and therefore have the risk of overfitting because the tree structure will adapt itself to the training data. Such a model is called **nonparametric** because the number of parameters is not predetermined prior to training, so the model structure is free to stick closely to the data. \n",
    "\n",
    "In contrast, a **parametric** model (eg. linear model) has a predetermined number of parameters, so its degree of freedom is limited, reducing the risk of overfitting but increasing the risk of underfitting. \n",
    "\n",
    "To avoid overfitting the training data, we can regularize Decision Trees: \n",
    "\n",
    " * Reducing **max_depth** regularizes the model \n",
    " * Increasing **min_samples_split** (minimum number of samples a node must have before it can be split), **min_samples_leaf** (minimum number of samples a leaf node must have), **min_weight_fraction_leaf** regularizes the model\n",
    " * Reducing **max_leaf_nodes** (maximum number of leaf nodes) and **max_features** (maximum number of features that are evaluated for splitting each node) regularizes the model \n",
    " \n",
    "\n",
    "**Pruning** \n",
    "\n",
    "Alternatively, other algorithms work by first training a Decision Tree without restrictions and then pruning unnecessary nodes. --> $\\chi^{2}$ test is used to estimate the probability that the improvement due to pruning is purely the result of chance (null hypothesis). If this probability (p-value) is higher than a given threshold, then the node is considered unnecessary and its children are deleted. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rising-skirt",
   "metadata": {},
   "source": [
    "#### Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "hybrid-juvenile",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor \n",
    "\n",
    "tree_reg = DecisionTreeRegressor(max_depth = 2)\n",
    "tree_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimum-uganda",
   "metadata": {},
   "source": [
    "The prediction is the average target value of the training instances associated with the leaf node of interest. \n",
    "\n",
    "**Cost function for regression**\n",
    "\n",
    "$J(k, t_{k})$ = $\\frac{m_{left}}{m}$ $MSE_{left}$ + $\\frac{m_{right}}{m}$ $MSE_{right}$\n",
    "\n",
    "Instead of trying to split the training set in a way that minimizes impurity, it now tries to split the training set in a way that minimizes the MSE. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intense-blend",
   "metadata": {},
   "source": [
    "#### Instability \n",
    "\n",
    "Decision trees like orthogonal decision boundaries, which makes them sensitive to training set rotation. One way to limit this problem is to use **Principal Component Analysis**, which results in a better orientation of the training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statistical-contest",
   "metadata": {},
   "source": [
    "#### End of notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-enclosure",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scanpy_env",
   "language": "python",
   "name": "scanpy_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
