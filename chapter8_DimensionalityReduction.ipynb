{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "apparent-partner",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nuclear-raise",
   "metadata": {},
   "source": [
    "*Curse of dimensionality* : large number of features makes the training slow and can make it hard to find a good solution. This is because high-dimensional datasets are at risk of being very **sparse** and therefore the **training instances are likely to be far away from each other**. When we have to make predictions on a new instance, this will likely be far away from each training instance, thus making the prediction based on large **extrapolation**. \n",
    "\n",
    "--> **Reducing dimensionality** does cause some information loss but significantly speeds up training and can filter out noise. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arctic-darkness",
   "metadata": {},
   "source": [
    "### 1. Approaches for dimensionality reduction\n",
    "\n",
    "#### 1a. Projection \n",
    "\n",
    "In most cases, training instances are **not spread out uniformly** across all dimensions due to: \n",
    "\n",
    " * some features being almost constant \n",
    " * some features being correlated \n",
    " \n",
    "--> as a result, all training instances **lie within a lower-dimensional subspace** of the high-dimensional space. \n",
    "\n",
    "**Project every training instance perpendicularly onto this subspace** \n",
    "\n",
    "\n",
    "#### 1b. Manifold Learning \n",
    "\n",
    "If the subspace twists and turns, such as the famous Swiss roll case, then simply projecting onto a plane would squash different layers together. Instead, we want to \"unroll\" it. \n",
    "\n",
    "**d-dimensional manifold is a part of an n-dimensional space (where d < n) that locally resembles a d-dimensional hyperplane**. Manifold learning relies on the manifold hypothesis = high-dimensional datasets lie close to a much lower-dimensional manifold. It attempts at modeling the manifold on which the training instances lie. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driven-thesis",
   "metadata": {},
   "source": [
    "### 2. Principal Component Analysis (Projection method)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visible-aaron",
   "metadata": {},
   "source": [
    " * Identifies the hyperplane that lies closest to the data: \n",
    "     1. Select the axis that preserves the maximum amount of variance (therefore likely to lose less information) or that minimizes the mean squared distance between the original dataset and its projection onto that axis. \n",
    "     2. Select a second axis, orthogonal to the first, that accounts for the largest amount of the remaining variance, \n",
    "     3. Iterate the procedure to find as many axes as the number of dimensions in the dataset. \n",
    "\n",
    "The i-th axis is called the i-th principal component of the data. \n",
    "For each principal component, PCA finds a 0-centered unit vector pointing in the direction of the PC. However, since two opposing unit vectors lie on the same axis, the direction of the unit vectors returned by PCA is not stable, thought the plane they define remains the same. \n",
    "\n",
    "**Singular Value Decomposition** : matrix factorization technique that can decompose the training set matrix $X$ into the matrix multiplication of 3 matrices: $U$ $\\Sigma$ $V^{T}$ \n",
    "\n",
    "--> $V$ contains the unit vectors that define all the principal components we are looking for. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bottom-chile",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris \n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "identified-cycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training dataset into train and test sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X,\n",
    "        y,\n",
    "        test_size=0.3, # This can be changed, though it makes sense to use 25-30% of the data for test\n",
    "        random_state=1996\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dirty-storage",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "X_centered = X - X.mean(axis = 0)\n",
    "U, s, Vt = np.linalg.svd(X_centered)\n",
    "c1 = Vt.T[:, 0]\n",
    "c2 = Vt.T[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unsigned-career",
   "metadata": {},
   "source": [
    "**!** PCA assumes the dataset is centered around the origin! Though sklearn's PCA classes take care of this themselves "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marine-bangkok",
   "metadata": {},
   "source": [
    " * Projects the data onto the hyperplane defined by the first d principal components \n",
    " \n",
    "$W_{d}$ = matrix containing the first d columns of $V$ \n",
    "\n",
    "--> Projection: $X_{d-proj}$ = $X$ $W_{d}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "endangered-standing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "devoted-healthcare",
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = Vt.T[:, 2]\n",
    "X2D = X_centered.dot(W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "annoying-workstation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with sklearn \n",
    "from sklearn.decomposition import PCA \n",
    "\n",
    "pca = PCA(n_components = 2)\n",
    "X2D = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt-energy",
   "metadata": {},
   "source": [
    " * Explained variance ratio of each principal components indicates the proportion of variance that lies along each principal component "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "mechanical-thompson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.92461872, 0.05306648])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aboriginal-pipeline",
   "metadata": {},
   "source": [
    " * Choosing the right number of dimensions by specifying the total amount of variance that we want to retain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "tutorial-agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 0.95)\n",
    "X_reduced = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "later-colon",
   "metadata": {},
   "source": [
    " * PCA for compression --> reconstruction error = mean squared distance between the original data and the reconstructed (compressed and then decompressed) data\n",
    " \n",
    "$X_{recovered}$ = $X_{d-proj}$ $W_{d}^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "american-spiritual",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_recovered = pca.inverse_transform(X_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efficient-montreal",
   "metadata": {},
   "source": [
    "#### 2a. Randomized PCA\n",
    "\n",
    "Algorithm that quickly find an approximation of the first d principal components with faster and more efficient computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "diverse-mining",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_pca = PCA(n_components = 2, svd_solver = \"randomized\") # specify the solver \n",
    "X_reduced = rnd_pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "macro-pierce",
   "metadata": {},
   "source": [
    "#### 2b. Incremental PCA \n",
    "\n",
    "Allow to split the training dataset into mini-batches and feed an IPCA algorithm one mini-batch at a time. \n",
    "Useful for:\n",
    "\n",
    " * Large training sets\n",
    " * Online learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "unknown-draft",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA \n",
    "\n",
    "n_batches = 10\n",
    "inc_pca = IncrementalPCA(n_components = 2)\n",
    "for X_batch in np.array_split(X_train, n_batches):\n",
    "    inc_pca.partial_fit(X_batch)\n",
    "    \n",
    "X_reduced = inc_pca.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stylish-devon",
   "metadata": {},
   "source": [
    "#### 2c. Kernel PCA\n",
    "\n",
    "**kernel trick** = mathematical technique that implicitly maps instances into a very high-dimensional space (called the feature space), enabling nonlinear classification and regression with Support Vector Machines. --> a linear decision boundary in the high-dimensional feature space corresponds to a complex nonlinear decision boundary in the original space. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "democratic-quarterly",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA \n",
    "\n",
    "rbf_pca = KernelPCA(n_components = 2, kernel = 'rbf', gamma = 0.04)\n",
    "X_reduced = rbf_pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rural-belle",
   "metadata": {},
   "source": [
    "### 3. Manifold learning methods\n",
    "\n",
    "#### 3a. Locally Linear Embedding\n",
    "\n",
    " * First measures how each training instance linearly relates to its closest neighbors\n",
    " * Looks for a low-dimensional representation of the training set where these locacl relationships are best preserved \n",
    " \n",
    "For each training instance $x^{(i)}$:\n",
    " 1. Identify the k nearest neighbors\n",
    " 2. Reconstruct $x^{(i)}$ as a linear function of its neighbors: finds the weights $w_{i,i}$ such that the squared distance between  $x^{(i)}$ and $\\sum_{j = 1}^{m} w_{i,i} x^{(i)}$ is as small as possible. \n",
    " 3. Constrain by normalizing the weights of each training instance $x^{(i)}$\n",
    " 4. Map the training instances to a d-dimensional space while preserving the local relationships (weight matrix) as much as possible\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "israeli-affair",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "\n",
    "lle = LocallyLinearEmbedding(n_components = 2, n_neighbors = 10)\n",
    "X_reduced = lle.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fuzzy-accordance",
   "metadata": {},
   "source": [
    "#### End of notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expressed-albany",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scanpy_env",
   "language": "python",
   "name": "scanpy_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
