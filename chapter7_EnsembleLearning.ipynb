{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "particular-neighborhood",
   "metadata": {},
   "source": [
    "### Ensemble Learning and Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virtual-frame",
   "metadata": {},
   "source": [
    "*Wisdom of the crowd* idea: if we aggregate a group of predictors, we get better predictions than with the best individual predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "located-customer",
   "metadata": {},
   "source": [
    "#### 1. Voting Classifiers\n",
    "\n",
    "**Hard voting** classifier: aggregate the predictions of each classifier and predict the class that gets the most votes. \n",
    "\n",
    "**!** Ensemble methods work best when the predictors are as independent from one another as possible. This can be achieved by training the classifiers with different algorithms, so to ensure that they will make different types of errors, improving the ensemble's accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "packed-enlargement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use moons dataset \n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_moons(n_samples = 100, noise = 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "saved-frequency",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training dataset into train and test sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X,\n",
    "        y,\n",
    "        test_size=0.3, # This can be changed, though it makes sense to use 25-30% of the data for test\n",
    "        random_state=1996\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dependent-circus",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "violent-algorithm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate individual classifiers\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "wrong-ending",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression()),\n",
       "                             ('rf', RandomForestClassifier()), ('svc', SVC())])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf = VotingClassifier(\n",
    "    estimators = [('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting = 'hard'\n",
    ")\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "alien-center",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.9\n",
      "RandomForestClassifier 1.0\n",
      "SVC 0.9666666666666667\n",
      "VotingClassifier 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-indication",
   "metadata": {},
   "source": [
    "If all classifiers are able to estimate class probabilities (predict_proba() method) then the Ensemble classifier can predict the class with the highest class probability averaged over all the individual classifiers (**soft voting**) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "revolutionary-syracuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate individual classifiers\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC(probability = True) # set to False by default in SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "velvet-disaster",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression()),\n",
       "                             ('rf', RandomForestClassifier()),\n",
       "                             ('svc', SVC(probability=True))],\n",
       "                 voting='soft')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf = VotingClassifier(\n",
    "    estimators = [('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting = 'soft'\n",
    ")\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "imported-prompt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.9\n",
      "RandomForestClassifier 1.0\n",
      "SVC 0.9666666666666667\n",
      "VotingClassifier 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integrated-accent",
   "metadata": {},
   "source": [
    "#### 2. Bagging and Pasting\n",
    "\n",
    "Instead of using different training algorithms, we can instead use the same training algorithm for every predictor but train them on a different random subset of the training set. \n",
    "\n",
    "If sampling is performed **with replacement** --> **BAGGING** (short for bootstrap aggregating)\n",
    "If sampling is performed **without replacement** --> **PASTING** \n",
    "\n",
    "Once all predictors are trained, the ensemble can make a prediction for a new instance by simply aggregating the predictions of all predictors. The aggregation function is usually:\n",
    "\n",
    " * **statistical mode** (most frequent prediction, like a hard voting classifier) for classification\n",
    " * **average** for regression \n",
    " \n",
    "Each individual predictor has a higher bias than if it were trained on the original training set, but the aggregation reduces both bias and variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "guided-receipt",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier \n",
    "from sklearn.tree import DecisionTreeClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "combined-moore",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "DecisionTreeClassifier(), n_estimators = 500, max_samples = 50, bootstrap = True, n_jobs = -1)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "announced-champion",
   "metadata": {},
   "source": [
    "#### Out-of-bag evaluation \n",
    "\n",
    "With bagging, some instances may be sampled several times for any given predictor, while others may not be sampled at all --> the training instances that are not sampled are called **out-of-bag** instances. Therefore a bagging ensemble can be evaluated using out-of-bag instances without the need for a separate validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "horizontal-factory",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9285714285714286"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "DecisionTreeClassifier(), n_estimators = 500, max_samples = 50, bootstrap = True, n_jobs = -1,\n",
    "oob_score = True # request an automatic out-of-bag evaluation after training\n",
    ")\n",
    "bag_clf.fit(X_train, y_train)\n",
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "victorian-earth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = bag_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "challenging-training",
   "metadata": {},
   "source": [
    "#### Random Patches and Random Subspaces "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interim-trustee",
   "metadata": {},
   "source": [
    "BaggingClassifier class supports sampling of features as well controlled by two hyperparameters: \n",
    "\n",
    " * max_features \n",
    " * bootstrap_features\n",
    " \n",
    "\n",
    "This technique is very useful when dealing with high-dimensional inputs (e.g. images). Sampling both training instances and features is called **Random Patches** method. \n",
    "\n",
    "Keeping all training instances (by setting bootstrap = False and max_samples = 1.0) but sampling features is instead called the **Random Subspaces** method. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divided-current",
   "metadata": {},
   "source": [
    "#### 3. Random Forests\n",
    "\n",
    "Random Forest is an ensemble of Decision Trees, generally trained via the bagging method with max_samples set to the size of the training set. It has all the hyperparameters of the DecisionTreeClassifier plus all the hyperparameters of a BaggingClassifier to control the ensemble itself.\n",
    "\n",
    " * Introduces extra randomness when growing trees by searching for the best feature among a random subset of features when splitting a node\n",
    " * Greater tree diversity --> trades a higher bias for a lower variance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "obvious-astrology",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_clf = RandomForestClassifier(n_estimators = 500, max_leaf_nodes = 16, n_jobs = -1)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "y_pred_rf = rnd_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiac-stations",
   "metadata": {},
   "source": [
    "#### Feature importance \n",
    "\n",
    "Measure a feature's importance by looking at how much the tree nodes that use that feature reduce impurity on average across all trees in the forest (weighted average). Sklearn computes this score automatically for each feature after training, then it scales the results so that the sum of all importances is equal to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "valuable-midwest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.08700477823573695\n",
      "sepal width (cm) 0.024017209601911817\n",
      "petal length (cm) 0.4517276004598349\n",
      "petal width (cm) 0.4372504117025165\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris \n",
    "\n",
    "iris = load_iris()\n",
    "rnd_clf = RandomForestClassifier(n_estimators = 500, n_jobs = -1)\n",
    "rnd_clf.fit(iris['data'], iris['target'])\n",
    "for name, score in zip(iris['feature_names'], rnd_clf.feature_importances_):\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aware-thesaurus",
   "metadata": {},
   "source": [
    "#### 4. Boosting\n",
    "\n",
    "Boosting refers to any Ensemble method that can combine several weak learners into a strong learner. The idea is to train predictors sequentially, each trying to correct its predecessor. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obvious-equilibrium",
   "metadata": {},
   "source": [
    "#### 4a. AdaBoost (Adaptive Boosting)\n",
    "\n",
    "A new predictor corrects its predecssor by paying more attention to the training instances that the predecessor underfitted --> results in new predictors focussing more and more on the hard cases. \n",
    "\n",
    " 1. Train first a base classifier \n",
    " 2. Use the base classifier to make predictions on the training set \n",
    " 3. Increase the relative weight of misclassified training instances \n",
    " 4. Train a second classifier using the updated weights\n",
    " 5. Repeat \n",
    " \n",
    "\n",
    "Once all predictors are trained, the ensemble makes predictions like bagging or pasting, but in AdaBoost the predictors have different weights depending on their overall accuracy on the weighted training set. \n",
    "\n",
    "**! Drawback**: AdaBoost cannot be parallelized since each predictor can only be trained after the previous predictor has been trained and evaluated. \n",
    "\n",
    " * Each instance weight $w^{(i)}$ is initially set to $\\frac{1}{m}$\n",
    " * A first predictor is trained, and its weighted error rate $r_{i}$ is computed on the training set \n",
    " \n",
    "\n",
    "**Weighted error rate of the j-th predictor**\n",
    "\n",
    "$r_{j}$ = $\\frac{\\sum_{i = 1}^{m} w_{(i)}}{\\sum_{i = 1}^{m} w_{(i)}}$ for $\\hat{y}_{j}^{(i)}$ != $y^{(i)}$\n",
    "\n",
    " * $\\hat{y}_{j}^{(i)}$ = j-th predictor's prediction for the i-th instance \n",
    "\n",
    "\n",
    "**Predictor weight**\n",
    "\n",
    "$\\alpha_{j}$ = $\\eta log \\frac{1 - r_{j}}{r_{j}}$\n",
    "\n",
    " * The more accurate the predictor is, the higher its weight \n",
    " * Update the instance weights, which boots the weights of the misclassified instances \n",
    "\n",
    "\n",
    "**Weight update rule**\n",
    "\n",
    "$w^{(i)}$ = \n",
    "\n",
    " * $w^{(i)}$ if $\\hat{y}_{j}^{(i)}$ = $y^{(i)}$\n",
    "\n",
    " * $w^{(i)}$ $exp(\\alpha_{j})$ if $\\hat{y}_{j}^{(i)}$ != $y^{(i)}$\n",
    " \n",
    "The algorithm stops when the desired number of predictors is reached or when a perfect predictor is found. \n",
    "\n",
    "**AdaBoost predictions** \n",
    "\n",
    "$\\hat{y}(x)$ = $argmax_{k}$ $\\sum_{j = 1}^{N} \\alpha_{j}$ for $\\hat{y}_{j}(x)$ = $k$\n",
    "\n",
    " * N = number of predictors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "reliable-local",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
       "                   learning_rate=0.5, n_estimators=200)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier \n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "DecisionTreeClassifier(max_depth = 1), n_estimators = 200, \n",
    "algorithm = 'SAMME.R', learning_rate = 0.5)\n",
    "ada_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forbidden-simon",
   "metadata": {},
   "source": [
    "sklearn implementation uses **Decision Stumps**, which are Decision Trees with a max_depth = 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "later-dancing",
   "metadata": {},
   "source": [
    "#### 4b. Gradient Boosting\n",
    "\n",
    "Like AdaBoost, Gradient Boosting works by sequentially adding predictors to an ensemble, each one correcting its predecessor. \n",
    "\n",
    "**!** Instead of tweaking the instance weights at every iteration, it tries to fit the new predictor to the residual errors made by the previous predictor. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "precise-perth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor \n",
    "\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth = 2)\n",
    "tree_reg1.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "electronic-relaxation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the second DecisionTreeRegressor on the residual errors made by the first predictor\n",
    "y2 = y - tree_reg1.predict(X)\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth = 2)\n",
    "tree_reg2.fit(X, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "finished-fraction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the third DecisionTreeRegressor on the resiudal errors made by the second predictor\n",
    "y3 = y2 - tree_reg2.predict(X)\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth = 2)\n",
    "tree_reg3.fit(X, y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "variable-cache",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction on a new instance by adding up the predictions of all the trees \n",
    "y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "natural-vector",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=1.0, max_depth=2, n_estimators=3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Automatically done by sklearn with the class GradientBoostingRegressor \n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor \n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth = 2, n_estimators = 3, learning_rate = 1.0)\n",
    "gbrt.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decimal-youth",
   "metadata": {},
   "source": [
    "**Shrinkage Regularization**\n",
    "\n",
    "Learning rate scales the contribution of each tree: if low value (e.g. 0.1), we will need more trees in the ensemble to fit the training set, but the predictions will generalize better. \n",
    "\n",
    "**Early stopping** \n",
    "\n",
    "To find the optimal number of trees we can use early stopping with the staged_predict() method, which returns an iterator over the predictions made by the ensemble at each stage of training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "impressive-magnet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(max_depth=2, n_estimators=118)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "from sklearn.metrics import mean_squared_error \n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth = 2, n_estimators = 120)\n",
    "gbrt.fit(X_train, y_train)\n",
    "\n",
    "errors = [mean_squared_error(y_test, y_pred) for y_pred in gbrt.staged_predict(X_test)]\n",
    "best_n_estimators = np.argmin(errors) + 1\n",
    "\n",
    "gbrt_best = GradientBoostingRegressor(max_depth = 2, n_estimators = best_n_estimators)\n",
    "gbrt_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attached-panama",
   "metadata": {},
   "source": [
    "**Stochastic Gradient Boosting** \n",
    "\n",
    "Specify the fraction of training instances to be used for training each tree with subsample hyperparameter. This trades a higher bias for a lower variance and speeds up training. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gorgeous-default",
   "metadata": {},
   "source": [
    "**!** An optimized implementation of Gradient Boosting is available in the python library **XGBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "daily-quarter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "\n",
    "xgb_reg = xgboost.XGBRegressor()\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "y_pred = xgb_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "surgical-rainbow",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:0.36873\n",
      "[1]\tvalidation_0-rmse:0.26813\n",
      "[2]\tvalidation_0-rmse:0.19520\n",
      "[3]\tvalidation_0-rmse:0.14229\n",
      "[4]\tvalidation_0-rmse:0.10389\n",
      "[5]\tvalidation_0-rmse:0.07617\n",
      "[6]\tvalidation_0-rmse:0.05635\n",
      "[7]\tvalidation_0-rmse:0.04064\n",
      "[8]\tvalidation_0-rmse:0.02908\n",
      "[9]\tvalidation_0-rmse:0.02178\n",
      "[10]\tvalidation_0-rmse:0.01739\n",
      "[11]\tvalidation_0-rmse:0.01309\n",
      "[12]\tvalidation_0-rmse:0.00992\n",
      "[13]\tvalidation_0-rmse:0.00757\n",
      "[14]\tvalidation_0-rmse:0.00583\n",
      "[15]\tvalidation_0-rmse:0.00490\n",
      "[16]\tvalidation_0-rmse:0.00432\n",
      "[17]\tvalidation_0-rmse:0.00399\n",
      "[18]\tvalidation_0-rmse:0.00378\n",
      "[19]\tvalidation_0-rmse:0.00380\n"
     ]
    }
   ],
   "source": [
    "# Automatically takes care of early stopping\n",
    "xgb_reg.fit(X_train, y_train, eval_set = [(X_test, y_test)], early_stopping_rounds = 2)\n",
    "y_pred = xgb_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recreational-consensus",
   "metadata": {},
   "source": [
    "#### 5. Stacking\n",
    "\n",
    "Instead of using functions to aggregate the predictions of all predictors in an ensemble, train a model to perform this aggregation. Such model is called a blender or meta-learner, which takes the predictions outputted by the predictors and makes the final prediction. \n",
    "\n",
    " * To train the blender we can use a hold-out set\n",
    "\n",
    "Open source implementation: DESlib "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boolean-quilt",
   "metadata": {},
   "source": [
    "#### End of notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noticed-natural",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scanpy_env",
   "language": "python",
   "name": "scanpy_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
